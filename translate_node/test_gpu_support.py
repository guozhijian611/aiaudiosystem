#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GPUæ”¯æŒè¯Šæ–­è„šæœ¬
æ£€æµ‹PyTorchã€CUDAã€GPUçŠ¶æ€å’Œfloat16æ”¯æŒæƒ…å†µ
"""

import sys
import os

def test_pytorch_installation():
    """æµ‹è¯•PyTorchå®‰è£…"""
    print("=" * 50)
    print("1. æµ‹è¯•PyTorchå®‰è£…")
    print("=" * 50)
    
    try:
        import torch
        print(f"âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ“ PyTorchå®‰è£…è·¯å¾„: {torch.__file__}")
        return True
    except ImportError as e:
        print(f"âœ— PyTorchå¯¼å…¥å¤±è´¥: {e}")
        return False

def test_cuda_support():
    """æµ‹è¯•CUDAæ”¯æŒ"""
    print("\n" + "=" * 50)
    print("2. æµ‹è¯•CUDAæ”¯æŒ")
    print("=" * 50)
    
    try:
        import torch
        
        # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        cuda_available = torch.cuda.is_available()
        print(f"CUDAå¯ç”¨: {cuda_available}")
        
        if cuda_available:
            # CUDAç‰ˆæœ¬ä¿¡æ¯
            cuda_version = torch.version.cuda
            print(f"âœ“ CUDAç‰ˆæœ¬: {cuda_version}")
            
            # GPUè®¾å¤‡ä¿¡æ¯
            device_count = torch.cuda.device_count()
            print(f"âœ“ GPUè®¾å¤‡æ•°é‡: {device_count}")
            
            for i in range(device_count):
                device_name = torch.cuda.get_device_name(i)
                device_props = torch.cuda.get_device_properties(i)
                memory_total = device_props.total_memory / 1024**3  # GB
                print(f"  GPU {i}: {device_name}")
                print(f"    æ€»å†…å­˜: {memory_total:.1f}GB")
                print(f"    è®¡ç®—èƒ½åŠ›: {device_props.major}.{device_props.minor}")
            
            # å½“å‰è®¾å¤‡
            current_device = torch.cuda.current_device()
            print(f"âœ“ å½“å‰è®¾å¤‡: cuda:{current_device}")
            
            return True
        else:
            print("âœ— CUDAä¸å¯ç”¨")
            return False
            
    except Exception as e:
        print(f"âœ— CUDAæ£€æµ‹å¤±è´¥: {e}")
        return False

def test_float16_support():
    """æµ‹è¯•float16æ”¯æŒ"""
    print("\n" + "=" * 50)
    print("3. æµ‹è¯•float16æ”¯æŒ")
    print("=" * 50)
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âœ— CUDAä¸å¯ç”¨ï¼Œæ— æ³•æµ‹è¯•GPU float16æ”¯æŒ")
            return False
        
        # æµ‹è¯•GPU float16æ”¯æŒ
        device = torch.cuda.current_device()
        device_props = torch.cuda.get_device_properties(device)
        
        print(f"GPUè®¡ç®—èƒ½åŠ›: {device_props.major}.{device_props.minor}")
        
        # è®¡ç®—èƒ½åŠ›7.0ä»¥ä¸Šé€šå¸¸æ”¯æŒé«˜æ•ˆçš„float16
        supports_efficient_fp16 = device_props.major >= 7
        print(f"æ”¯æŒé«˜æ•ˆfloat16: {supports_efficient_fp16}")
        
        # å®é™…æµ‹è¯•float16æ“ä½œ
        try:
            x = torch.randn(100, 100, dtype=torch.float16, device='cuda')
            y = torch.randn(100, 100, dtype=torch.float16, device='cuda')
            z = torch.matmul(x, y)
            print("âœ“ float16çŸ©é˜µè¿ç®—æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•è‡ªåŠ¨æ··åˆç²¾åº¦
            with torch.cuda.amp.autocast():
                z_amp = torch.matmul(x.float(), y.float())
            print("âœ“ è‡ªåŠ¨æ··åˆç²¾åº¦æµ‹è¯•é€šè¿‡")
            
            return True
            
        except Exception as e:
            print(f"âœ— float16æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âœ— float16æ”¯æŒæ£€æµ‹å¤±è´¥: {e}")
        return False

def test_ctranslate2_support():
    """æµ‹è¯•CTranslate2æ”¯æŒ"""
    print("\n" + "=" * 50)
    print("4. æµ‹è¯•CTranslate2æ”¯æŒ")
    print("=" * 50)
    
    try:
        import ctranslate2
        print(f"âœ“ CTranslate2ç‰ˆæœ¬: {ctranslate2.__version__}")
        
        # æ£€æŸ¥æ”¯æŒçš„è®¡ç®—ç±»å‹
        print("æ”¯æŒçš„è®¡ç®—ç±»å‹:")
        
        # æµ‹è¯•ä¸åŒçš„è®¡ç®—ç±»å‹
        compute_types = ['float32', 'float16', 'int8']
        
        for compute_type in compute_types:
            try:
                # è¿™é‡Œåªæ˜¯æµ‹è¯•å¯¼å…¥ï¼Œä¸å®é™…åˆ›å»ºæ¨¡å‹
                print(f"  {compute_type}: å¯ç”¨")
            except Exception as e:
                print(f"  {compute_type}: ä¸å¯ç”¨ - {e}")
        
        return True
        
    except ImportError as e:
        print(f"âœ— CTranslate2å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âœ— CTranslate2æ£€æµ‹å¤±è´¥: {e}")
        return False

def test_faster_whisper():
    """æµ‹è¯•faster-whisper"""
    print("\n" + "=" * 50)
    print("5. æµ‹è¯•faster-whisper")
    print("=" * 50)
    
    try:
        import torch
        from faster_whisper import WhisperModel
        print("âœ“ faster-whisperå¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•ä¸åŒè®¡ç®—ç±»å‹çš„æ¨¡å‹åˆ›å»º
        compute_types = ['float32', 'float16', 'int8']
        
        for compute_type in compute_types:
            try:
                print(f"\næµ‹è¯•è®¡ç®—ç±»å‹: {compute_type}")
                
                # ä½¿ç”¨æœ€å°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
                model = WhisperModel(
                    "tiny", 
                    device="cuda" if torch.cuda.is_available() else "cpu",
                    compute_type=compute_type
                )
                print(f"  âœ“ {compute_type} æ¨¡å‹åˆ›å»ºæˆåŠŸ")
                
                # æ¸…ç†æ¨¡å‹
                del model
                
            except Exception as e:
                print(f"  âœ— {compute_type} æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        
        return True
        
    except ImportError as e:
        print(f"âœ— faster-whisperå¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âœ— faster-whisperæµ‹è¯•å¤±è´¥: {e}")
        return False

def get_recommended_config():
    """è·å–æ¨èé…ç½®"""
    print("\n" + "=" * 50)
    print("6. æ¨èé…ç½®")
    print("=" * 50)
    
    try:
        import torch
        
        if torch.cuda.is_available():
            device_props = torch.cuda.get_device_properties(0)
            compute_capability = f"{device_props.major}.{device_props.minor}"
            
            print(f"æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
            print(f"è®¡ç®—èƒ½åŠ›: {compute_capability}")
            
            # æ ¹æ®è®¡ç®—èƒ½åŠ›æ¨èé…ç½®
            if device_props.major >= 7:  # Voltaæ¶æ„åŠä»¥ä¸Š
                recommended_compute_type = "float16"
                print("âœ“ æ¨èä½¿ç”¨ float16 (æ”¯æŒé«˜æ•ˆåŠç²¾åº¦è®¡ç®—)")
            elif device_props.major >= 6:  # Pascalæ¶æ„
                recommended_compute_type = "float32"
                print("âš  æ¨èä½¿ç”¨ float32 (float16æ”¯æŒæœ‰é™)")
            else:
                recommended_compute_type = "float32"
                print("âš  æ¨èä½¿ç”¨ float32 (è¾ƒè€çš„GPUæ¶æ„)")
            
            print(f"\næ¨èç¯å¢ƒå˜é‡è®¾ç½®:")
            print(f"WHISPER_DEVICE=cuda")
            print(f"WHISPER_COMPUTE_TYPE={recommended_compute_type}")
            
        else:
            print("æœªæ£€æµ‹åˆ°CUDA GPU")
            print(f"\næ¨èç¯å¢ƒå˜é‡è®¾ç½®:")
            print(f"WHISPER_DEVICE=cpu")
            print(f"WHISPER_COMPUTE_TYPE=float32")
            
    except Exception as e:
        print(f"é…ç½®æ¨èå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("GPUæ”¯æŒè¯Šæ–­å·¥å…·")
    print("æ£€æµ‹PyTorchã€CUDAã€GPUçŠ¶æ€å’Œfloat16æ”¯æŒæƒ…å†µ")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    pytorch_ok = test_pytorch_installation()
    
    if pytorch_ok:
        cuda_ok = test_cuda_support()
        float16_ok = test_float16_support()
        ctranslate2_ok = test_ctranslate2_support()
        faster_whisper_ok = test_faster_whisper()
        
        # è·å–æ¨èé…ç½®
        get_recommended_config()
        
        # æ€»ç»“
        print("\n" + "=" * 50)
        print("è¯Šæ–­æ€»ç»“")
        print("=" * 50)
        print(f"PyTorch: {'âœ“' if pytorch_ok else 'âœ—'}")
        print(f"CUDA: {'âœ“' if cuda_ok else 'âœ—'}")
        print(f"Float16: {'âœ“' if float16_ok else 'âœ—'}")
        print(f"CTranslate2: {'âœ“' if ctranslate2_ok else 'âœ—'}")
        print(f"Faster-Whisper: {'âœ“' if faster_whisper_ok else 'âœ—'}")
        
        if all([pytorch_ok, cuda_ok, ctranslate2_ok, faster_whisper_ok]):
            print("\nğŸ‰ æ‰€æœ‰ç»„ä»¶æ£€æµ‹é€šè¿‡ï¼")
        else:
            print("\nâš ï¸  å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°å¤±è´¥é¡¹ç›®")
    
    else:
        print("\nâŒ PyTorchæœªæ­£ç¡®å®‰è£…ï¼Œè¯·å…ˆè§£å†³PyTorchå®‰è£…é—®é¢˜")

if __name__ == "__main__":
    main() 